# Anal√≠tica Descriptiva

---

## 1. Evaluaci√≥n Inicial del Modelo

### üìå Revisar m√©tricas adecuadas para el problema

**Clasificaci√≥n**:

- *Accuracy: Porcentaje de predicciones correctas. √ötil solo en clases balanceadas.*
- *Precision: Proporci√≥n de predicciones positivas correctas (evitar falsos positivos).*
- *Recall: Proporci√≥n de casos positivos reales detectados (evitar falsos negativos).*
- *F1: Media arm√≥nica de precision y recall. Ideal para balancear ambas m√©tricas.*
- *ROC AUC: Eval√∫a la capacidad del modelo para distinguir entre clases (1 = perfecto, 0.5 = aleatorio).*
    
    ![image.png](image.png)
    
- PR AUC: *Eval√∫a el √°rea bajo la curva precision vs recall ‚Üí Eval√∫a qu√© tan bien el modelo identifica los positivos verdaderos, prestando atenci√≥n a los falsos positivos y falsos negativos.*
    
    ![image.png](image%201.png)
    

**Regresi√≥n**:

- *MSE (Error Cuadr√°tico Medio): Penaliza errores grandes (sensibilidad a outliers).*
- *RMSE: Ra√≠z de MSE. Interpretable en la misma unidad que la variable objetivo.*
- *MAE (Error Absoluto Medio): Promedio de errores absolutos (menos sensible a outliers).*
- *MAPE (Error Porcentual Absoluto Medio): Promedio del error absoluto expresado como porcentaje del valor real. Intuitivo para comunicar resultados en t√©rminos relativos (‚Äúel modelo se equivoca en promedio un 8 %‚Äù).*
- *R¬≤: Proporci√≥n de varianza explicada (1 = perfecto, 0 = modelo b√°sico).*

![image.png](image%202.png)

**Clustering:**

- *Silhouette Score: Mide cu√°n bien separado y compacto est√° cada cluster (-1 a 1)*
- *Davies‚ÄìBouldin Index (DBI): RPromedia la relaci√≥n entre la dispersi√≥n interna y la separaci√≥n entre clusters (0 a inf). Cuanto menor, mejor.*
- *Calinski‚ÄìHarabasz Index (CHI): Relaci√≥n entre la dispersi√≥n entre clusters y dentro de ellos. Cuanto mayor, mejor la separaci√≥n.*

<aside>
üìå

En clustering, la evaluaci√≥n depende de si existen etiquetas de referencia. En la mayor√≠a de los casos, se utilizan m√©tricas internas (como Silhouette o Davies‚ÄìBouldin) para comparar la calidad relativa entre diferentes modelos o cantidades de clusters.

</aside>

### ‚ö†Ô∏è Dataset Desbalanceado

- **Problema**: El **accuracy** puede ser enga√±oso (ej: 95% accuracy con 95% de casos en una clase).
- **Soluci√≥n**:
    - Usar **F1** o **ROC AUC/PR AUC** para evaluar el modelo.
    - Priorizar **recall** si es cr√≠tico detectar todos los positivos (ej: diagn√≥stico m√©dico).

### üîç Validar Overfitting/Underfitting

**C√≥mo detectarlo ‚Üí c**omparar m√©tricas en **train** vs **test**:

- **Overfitting**:
    - Ejemplo: Train accuracy = 0.98, Test accuracy = 0.65.
    - Causa: Modelo memoriza el entrenamiento y no generaliza.
- **Underfitting**:
    - Ejemplo: Train accuracy = 0.55, Test accuracy = 0.52.
    - Causa: Modelo es demasiado simple para capturar patrones.

![image.png](image%203.png)

![image.png](image%204.png)

**Acciones clave**:

- **Overfitting**: Reducir complejidad (regularizaci√≥n, menos *features*, m√°s datos).
- **Underfitting**: Aumentar complejidad (mejores *features*, modelo m√°s potente).

## 2. Diagn√≥stico: ¬øQu√© est√° fallando?

### üìä Desbalanceo de Clases

**üîç C√≥mo identificarlo:**

- Verificar la proporci√≥n entre clases (ej: 90% clase A vs. 10% clase B).
- Analizar la **matriz de confusi√≥n**:
    - Si el modelo tiene muchos falsos negativos o positivos en la clase minoritaria.

**üõ† Soluciones posibles:**

- **T√©cnicas de remuestreo**:
    - **Oversampling**: Generar m√°s ejemplos de la clase minoritaria (SMOTE).
    - **Undersampling**: Reducir ejemplos de la clase mayoritaria.
- **M√©tricas alternativas**: Usar **F1, Recall (sensibilidad) o PR AUC** en lugar de accuracy.
- **Pesos de clase**: Ajustar **`class_weight`** en modelos como LogisticRegression o RandomForest.

### üìâ Underfitting (Modelo demasiado simple)

**üîç S√≠ntomas:**

- Bajas m√©tricas tanto en **train** como en **test** (ej: accuracy ~50% en ambos).
- El modelo no capta patrones complejos en los datos.

**üõ† Soluciones posibles:**

1. **Modelos m√°s complejos**:
    - Cambiar de Regresi√≥n Log√≠stica a **RandomForest, XGBoost o Redes Neuronales**.
2. **Mejorar *features***:
    - A√±adir caracter√≠sticas relevantes o generar nuevas (*feature engineering*).
    - Usar transformaciones polin√≥micas o interacciones entre variables.
3. **Reducir regularizaci√≥n**: Si el modelo tiene par√°metros como **`C`** (en SVM) o **`alpha`** (en Ridge/Lasso), disminuirlos.

---

### üìà Overfitting (Modelo memoriza el ruido)

**üîç S√≠ntomas:**

- **Train** tiene m√©tricas altas (ej: 98%), pero **test** es bajo (ej: 70%).
- El modelo ajusta demasiado los detalles (incluyendo el ruido).

**üõ† Soluciones posibles:**

1. **Regularizaci√≥n:**
    - A√±adir penalizaciones (L1/Lasso, L2/Ridge) para simplificar el modelo.
    - Ajustar hiperpar√°metros como **`max_depth`** en √°rboles o **`min_samples_split`**.
2. **Reducir complejidad:**
    - Disminuir el n√∫mero de *features* (selecci√≥n con **`SelectKBest`** o an√°lisis de importancia).
    - Usar t√©cnicas de **pruning** en √°rboles de decisi√≥n.
3. **M√°s datos:**
    - Aumentar el dataset (data augmentation en im√°genes/texto).
4. **Validaci√≥n cruzada:**
    - Usar **`cross_val_score`** para asegurar que el modelo generaliza bien.

---

### üîé Resumen de Acciones por Problema

| **Problema** | **Diagn√≥stico** | **Posibles Soluciones** |
| --- | --- | --- |
| **Desbalanceo** | Clases desiguales, errores en minoritarias | Oversampling, F1/Recall, class\_weight |
| **Underfitting** | Mal desempe√±o en train y test | Modelos m√°s complejos, mejor feature engineering |
| **Overfitting** | Buen train, mal test | Regularizaci√≥n, simplificar modelo, m√°s datos |

## 3. Tratamiento del Desbalance

El desbalance de clases ocurre cuando una clase domina sobre las dem√°s, afectando el aprendizaje del modelo. Estas son las estrategias principales para manejarlo:

### üîÑ Re-muestreo

**T√©cnicas para ajustar la distribuci√≥n de clases:**

![image.png](image%205.png)

### ‚Üë Oversampling (Aumentar la clase minoritaria)

- **SMOTE (Synthetic Minority Over-sampling Technique):**
    - Genera ejemplos sint√©ticos interpolando datos de la clase minoritaria.
    - **Ventaja:** Evita la duplicaci√≥n exacta de datos.
    - **Limitaci√≥n:** Puede crear muestras poco realistas en espacios de alta dimensi√≥n.
        
        ![image.png](image%206.png)
        
- **ADASYN:**
    - Similar a SMOTE, pero enfocado en muestras dif√≠ciles de clasificar.
- **Duplicaci√≥n aleatoria:**
    - Simple pero puede causar sobreajuste si el modelo memoriza repeticiones.

### ‚Üì Undersampling (Reducir la clase mayoritaria)

- **Random Undersampling:**
    - Elimina aleatoriamente muestras de la clase mayoritaria.
    - **Riesgo:** P√©rdida de informaci√≥n valiosa.
- **T√©cnicas avanzadas:**
    - **Tomek Links:** Elimina muestras cercanas al l√≠mite entre clases.
        
        ![image.png](image%207.png)
        
    - **Cluster Centroids:** Reduce la mayor√≠a agrupando y conservando representantes.

### üîÑ Combinado (SMOTE + Tomek Links)

- Aplica *oversampling* en la minor√≠a y *undersampling* en la mayor√≠a.
- **Ventaja:** Balancea el dataset sin perder informaci√≥n cr√≠tica.

### ‚öñÔ∏è Ponderaci√≥n de Clases

**Alternativa al re-muestreo:** ajustar el peso de las clases en el modelo.

- **`class_weight='balanced'`:**
    - Asigna pesos inversamente proporcionales a la frecuencia de las clases.
    - **Modelos compatibles:** **`LogisticRegression`**, **`RandomForest`**, **`SVM`**.
- **Pesos personalizados:**
    - Ejemplo: **`class_weight={0: 1, 1: 10}`** (prioriza la clase **`1`**).

**¬øCu√°ndo usarlo?**

- Cuando el re-muestreo no es viable (ej.: datasets muy grandes).
- Para evitar el costo computacional de generar muestras sint√©ticas.

---

### üìä M√©tricas Adecuadas para Desbalance

**Evitar:** **`Accuracy`** (enga√±osa si una clase domina).

**Priorizar:**

- **F1-score:** Balance entre *precision* y *recall*. Ideal cuando ambas m√©tricas importan.
- **PR-AUC (Precision-Recall AUC):**
    - Mejor que ROC-AUC si hay muchas m√°s negativas que positivas.
    - Eval√∫a el rendimiento en la clase minoritaria.

**Ejemplo de uso:**

- En fraudes (clase positiva rara), optimizar para **F1 o Recall** (captar todos los casos).

---

### üîç Resumen: Estrategias vs. Contexto

| **T√©cnica** | **Ventajas** | **Cu√°ndo Usarla** |
| --- | --- | --- |
| **SMOTE** | Evita duplicados, mejora generalizaci√≥n | Datos tabulares, tama√±o moderado |
| **Undersampling** | R√°pido, reduce costo computacional | Dataset muy grande, mayor√≠a redundante |
| **Ponderaci√≥n** | Sin alterar datos, f√°cil de implementar | Modelos que soportan **`class_weight`** |
| **F1/PR-AUC** | Enfocado en la clase relevante | Evaluaci√≥n final tras ajustes |

**Siguiente paso:** Validar si las correcciones mejoraron el modelo (comparar m√©tricas antes/despu√©s).

## 4. Feature Engineering

El *feature engineering* es el proceso de crear, transformar y seleccionar variables para mejorar el rendimiento del modelo. Estas son las t√©cnicas clave:

### ‚ú® Creaci√≥n de Nuevas Features

**Objetivo:** Capturar patrones no evidentes en los datos crudos.

**Ejemplos comunes:**

- **Temporalidad:** Extraer *d√≠a de la semana*, *hora*, o *diferencia entre fechas*.
- **Agregaciones:** Promedios/m√°ximos por categor√≠a (ej: gasto promedio por cliente).
- **Ratios:** Proporciones entre variables (ej: ingresos/deudas).
- **Interacciones:** Combinar *features* (ej: **`edad * ingreso`**).

**Caso de uso:**

- En un modelo de *churn*, crear:
    - **`d√≠as_sin_actividad`** (fecha actual - √∫ltima compra).
    - **`gasto_promedio_√∫ltimos_3_meses`**.

### üîÑ Transformaciones

**Para distribuciones no lineales o *outliers*:**

- **Logaritmo (`np.log1p`)**:
    - Reduce el impacto de valores extremos (ej: ingresos).
- **Bucketizaci√≥n (Binning)**:
    - Convertir variables continuas en categ√≥ricas (ej: edades ‚Üí rangos).
- **Escalado:**
    - Normalizaci√≥n (**`MinMaxScaler`**) o estandarizaci√≥n (**`StandardScaler`**).
        
        ### `MinMaxScaler`
        
        **Qu√© hace:**
        
        - Transforma los datos para que est√©n en un **rango espec√≠fico** (por defecto, **`[0, 1]`**).
        - F√≥rmula:
        
        $$
        X_{\text{scaled}} = \frac{X - X_{\text{min}}}{X_{\text{max}} - X_{\text{min}}}
        $$
        
        **Caracter√≠sticas:**
        
        - **Ventaja:** √ötil cuando la distribuci√≥n no es normal o cuando necesitas que los datos est√©n acotados (ej.: im√°genes/p√≠xeles en **`[0, 1]`**).
        - **Desventaja:** Sensible a **outliers** (un valor extremo afecta el escalado de toda la columna).
        
        **Ejemplo de uso:**
        
        - Normalizar p√≠xeles de im√°genes (**`0-255`** ‚Üí **`0-1`**).
        - *Features* donde el l√≠mite m√≠nimo/m√°ximo tiene significado (ej.: porcentajes).
        
        ---
        
        ### `StandardScaler`
        
        **Qu√© hace:**
        
        - Estandariza los datos para que tengan **media = 0** y **desviaci√≥n est√°ndar = 1**.
        - F√≥rmula:
        
        $$
        X_{\text{scaled}} = \frac{X - \mu}{\sigma} \quad \text{(donde } \mu = \text{media, } \sigma = \text{desviaci√≥n est√°ndar)}
        $$
        
        **Caracter√≠sticas:**
        
        - **Ventaja:** Funciona bien con distribuciones normales (o cerca de lo normal) y es menos sensible a *outliers* que **`MinMaxScaler`**.
        - **Desventaja:** No garantiza un rango fijo (puede haber valores fuera de **`[-3, 3]`**).
        
        **Ejemplo de uso:**
        
        - Algoritmos que asumen distribuci√≥n normal (ej.: SVM, regresi√≥n lineal).
        - Datos con *outliers* moderados (la estandarizaci√≥n "suaviza" su impacto).
        
        ---
        
        ### üìå Comparaci√≥n Directa
        
        | **Criterio** | **`MinMaxScaler`** | **`StandardScaler`** |
        | --- | --- | --- |
        | **Rango** | **`[0, 1]`** (o personalizado) | Sin rango fijo (media=0, std=1) |
        | **Outliers** | Muy afectado | Menos afectado |
        | **Distribuci√≥n** | No requiere normalidad | Ideal para distribuciones normales |
        | **Uso t√≠pico** | Redes neuronales, im√°genes | SVM, PCA, modelos lineales |
        
        ---
        
        ### ¬øCu√°l elegir?
        
        - Usa **`MinMaxScaler`** si:
            - Necesitas un rango espec√≠fico (ej.: algoritmos que requieren *inputs* en **`[0, 1]`**).
            - Los datos no tienen *outliers* extremos.
        - Usa **`StandardScaler`** si:
            - Trabajas con modelos sensibles a la escala (SVM, regresi√≥n).
            - Hay *outliers*, pero no quieres que dominen el escalado.
        
        **Nota:** En duda, prueba ambos y compara el rendimiento del modelo.
        
        ---
        
        ### ¬øPod√©s entrenar sin escalar?
        
        S√≠, **pod√©s** entrenar sin escalar, y el modelo igual va a funcionar. Pero depende **qu√© tipo de modelo** uses ‚Üí Algunos algoritmos son **insensibles a la escala**, otros **la necesitan s√≠ o s√≠**.
        
        ---
        
        ### Modelos que **no necesitan escalado**
        
        Estos modelos se basan en **√°rboles de decisi√≥n o particiones del espacio**, por lo que **no les importa si una variable est√° en metros o en millones**:
        
        - `DecisionTreeClassifier` / `Regressor`
        - `RandomForest`
        - `XGBoost`, `LightGBM`, `CatBoost`
        
        **Ejemplo: s**i una variable vale 5 y otra 5000, el √°rbol usa ‚Äú>‚Äù o ‚Äú<‚Äù para dividir, no distancias. Entonces el rango num√©rico no afecta la l√≥gica.
        
        ---
        
        ### Modelos que **s√≠ necesitan escalado**
        
        Estos modelos usan **distancias, gradientes o magnitudes absolutas**: si las variables est√°n en escalas distintas, una domina a las otras.
        
        - Regresi√≥n lineal / log√≠stica
        - SVM (Support Vector Machine)
        - KNN (K-Nearest Neighbors)
        - PCA, t-SNE, K-Means, etc.
        - Redes neuronales (el escalado ayuda much√≠simo a la convergencia)
        
        Sin escalar, una variable con valores grandes puede ‚Äúpisar‚Äù a las dem√°s, y el modelo **pierde sensibilidad** frente a las que est√°n en rangos peque√±os.
        
        ---
        
        ### ¬øMejora la performance?
        
        Depende del modelo:
        
        | Tipo de modelo | Escalado necesario | Efecto del escalado |
        | --- | --- | --- |
        | √Årboles / Random Forest / XGBoost | ‚ùå No | Pr√°cticamente nulo |
        | Regresi√≥n lineal / log√≠stica | ‚úÖ S√≠ | Mejora estabilidad y convergencia |
        | SVM, KNN, K-Means | ‚úÖ S√≠ | Mejora precisi√≥n y reduce sesgos |
        | Redes neuronales | ‚úÖ Muy recomendado | Acelera el entrenamiento y evita saturaci√≥n |
        
        ---
        
        ### En resumen
        
        - **Pod√©s entrenar sin escalar**, pero en muchos modelos el resultado ser√° peor o inestable.
        - El escalado **no cambia la informaci√≥n**, pero **armoniza las magnitudes** para que los algoritmos comparen ‚Äúmanzanas con manzanas‚Äù.
        - En modelos basados en distancias o gradientes, **s√≠ mejora la performance y estabilidad**.

---

### üìä Encoding de Variables Categ√≥ricas

![image.png](1698513739094.jpg)

1. **One-Hot Encoding** (**`pd.get_dummies`**, **`OneHotEncoder`**):
    - **Qu√© hace:** Crea una columna binaria (0/1) por cada categor√≠a.
    - **Ejemplo:** Pa√≠s ‚Üí **`pa√≠s_Argentina`**, **`pa√≠s_Brasil`**, **`pa√≠s_Chile`**.
    - **Ventaja:** Sin p√©rdida de informaci√≥n.
    - **Desventaja:** "Explosi√≥n" de columnas si hay muchas categor√≠as (*curse of dimensionality*).
2. **Label Encoding** (**`sklearn.preprocessing.LabelEncoder`**):
    - **Qu√© hace:** Asigna un n√∫mero √∫nico a cada categor√≠a (ej: "Perro"=0, "Gato"=1, "P√°jaro"=2).
    - **Ejemplo:** Tama√±o ‚Üí **`"peque√±o"=0`**, **`"mediano"=1`**, **`"grande"=2`**.
    - **Ventaja:** Mantiene una sola columna.
    - **Desventaja:** **No es adecuado para categor√≠as sin orden**, ya que el modelo puede interpretar relaciones num√©ricas falsas (ej: "perro" < "gato").
3. **Ordinal Encoding** (**`OrdinalEncoder`**):
    - **Qu√© hace:** Similar a Label Encoding, pero con orden expl√≠cito (definido manualmente).
    - **Ejemplo:** Educaci√≥n ‚Üí **`"primaria"=1`**, **`"secundaria"=2`**, **`"universitario"=3`**.
    - **Ventaja:** Ideal para variables ordinales con jerarqu√≠a clara.
    - **Desventaja:** Requiere definir el orden manualmente.
4. **Target Encoding** (**`category_encoders.TargetEncoder`**):
    - **Qu√© hace:** Reemplaza cada categor√≠a con la media del *target* (ej: pa√≠s ‚Üí tasa promedio de churn).
    - **Ventaja:** Captura la relaci√≥n con la variable objetivo.
    - **Desventaja:** Riesgo de *overfitting* y *data leakage* (usar con validaci√≥n cruzada).

### üîç Tabla Comparativa Actualizada

| **M√©todo** | **Ventaja** | **Desventaja** | **Cu√°ndo Usarlo** |
| --- | --- | --- | --- |
| **One-Hot** | Sin p√©rdida de informaci√≥n | Aumenta dimensionalidad | Categor√≠as sin orden (nominales) |
| **Label Encoding** | Simple, una sola columna | Puede crear relaciones falsas | Solo si el orden no importa |
| **Ordinal Encoding** | Respeta orden natural | Requiere definir orden manual | Variables ordinales |
| **Target Encoding** | Captura relaci√≥n con el *target* | Riesgo de *overfitting* | Categor√≠as con alta cardinalidad |

### üí° Recomendaciones Clave

- **Para categor√≠as nominales** (sin orden): **One-Hot** (si pocas categor√≠as) o **Target Encoding** (si muchas).
- **Para categor√≠as ordinales**: **Ordinal Encoding**.
- **Label Encoding** solo si:
    - Es una variable nominal y usas modelos basados en √°rboles (Random Forest, XGBoost), pues estos no se ven afectados por asignaciones num√©ricas arbitrarias.
    - ¬°Nunca para modelos lineales o SVM!

---

### üöÄ Buenas Pr√°cticas

- **Validar impacto:** Comparar m√©tricas antes/despu√©s del *feature engineering*.
- **Evitar *data leakage*:** Calcular transformaciones (ej: Target Encoding) solo con datos de entrenamiento.
- **Priorizar interpretabilidad:** *Features* deben tener sentido en el contexto del problema.

---

### ¬øPod√©s entrenar sin codificar variables categ√≥ricas?

Depende **del tipo de modelo** (de nuevo). Los algoritmos num√©ricos **no entienden texto o categor√≠as directamente**, pero algunos modelos m√°s modernos **s√≠ pueden manejarlas internamente**.

### Modelos que **requieren encoding**

Estos modelos trabajan **solo con n√∫meros** y no pueden interpretar etiquetas de texto como ‚Äúrojo‚Äù, ‚Äúazul‚Äù, ‚Äúverde‚Äù:

- Regresi√≥n lineal / log√≠stica
- SVM
- KNN
- Redes neuronales
- K-Means, PCA
- Cualquier algoritmo de scikit-learn que calcule distancias o coeficientes

### Modelos que **no necesitan encoding expl√≠cito**

Algunos modelos de √°rboles modernos pueden **procesar variables categ√≥ricas directamente** o **manejar sus c√≥digos internos sin sesgo**:

- **CatBoost** ü•á (nativo para categor√≠as, las trata con codificaci√≥n estad√≠stica interna).
- **LightGBM** (permite columnas categ√≥ricas si las marc√°s con `categorical_feature`).
- **XGBoost** (recientemente, desde v1.6+ tiene soporte experimental para categor√≠as).

En estos casos, pod√©s pasar las columnas categ√≥ricas **sin one-hot**, y el modelo las maneja mejor y m√°s eficientemente.

### Resumen

| Tipo de modelo | ¬øNecesita encoding? | Tipo recomendado |
| --- | --- | --- |
| Regresi√≥n lineal / log√≠stica | ‚úÖ S√≠ | One-hot o Target |
| SVM / KNN / K-Means | ‚úÖ S√≠ | One-hot |
| Random Forest / Decision Tree | ‚ö†Ô∏è S√≠ (pero no cr√≠tico si us√°s LabelEncoder) |  |
| XGBoost / LightGBM | ‚öôÔ∏è Depende de la versi√≥n (s√≠ o nativo) |  |
| CatBoost | ‚ùå No (soporte nativo de categor√≠as) |  |
| Redes neuronales | ‚úÖ S√≠ | One-hot o embeddings |

---

### Selecci√≥n de Features

(Eliminar redundantes o poco relevantes).

### üîç M√©todos Comunes

1. **Eliminaci√≥n de Features con Baja Varianza**
    - **Qu√© hace**: Elimina columnas con valores constantes o casi constantes (no aportan informaci√≥n).
    - **Herramienta**: **`VarianceThreshold`** de scikit-learn.
2. **Correlaci√≥n entre Features**
    - **Qu√© hace**: Elimina variables altamente correlacionadas (redundantes).
    - **M√©todo**:
        - Calcular matriz de correlaci√≥n (**`df.corr()`**).
        - Eliminar una de cada par con correlaci√≥n > 0.9 (umbral ajustable).
3. **Selecci√≥n Univariante (Test Estad√≠sticos)**
    - **Qu√© hace**: Selecciona las *features* con mayor relaci√≥n estad√≠stica con el *target*.
    - **Herramientas**:
        - **`SelectKBest`**: Conserva las *K features* m√°s significativas.
        - Pruebas como **`chi2`** (clasificaci√≥n) o **`f_regression`** (regresi√≥n).
4. **Importancia de Features (Modelos Basados en √Årboles)**
    
    Este m√©todo utiliza algoritmos como *Random Forest* o *XGBoost* para **asignar un puntaje de importancia** a cada variable, indicando qu√© tan relevante es para predecir el *target*.
    
    **C√≥mo funciona:**
    
    - Los modelos de √°rboles **dividen los datos** en nodos basados en *features* que maximizan la pureza (ej: Gini, ganancia de informaci√≥n).
    - Las variables usadas en **divisiones tempranas o frecuentes** se consideran m√°s importantes.
    - Al final, se calcula un **promedio de importancia** en todos los √°rboles del *ensemble*.
    
    **Ventajas:**
    
    - Captura **relaciones no lineales** entre *features* y *target*.
    - No requiere escalado previo de los datos.
    - Proporciona una **interpretabilidad** clara (puedes rankear *features*).
    
    **Limitaciones:**
    
    - **Dependencia del modelo**: Si el modelo no es bueno, las importancias pueden ser enga√±osas.
    - **Puede favorecer *features* con muchos valores √∫nicos** (aunque no sean predictivos).
    
    **Cu√°ndo usarlo:**
    
    - Cuando trabajas con **datos tabulares** y modelos basados en √°rboles.
    - Para entender qu√© variables influyen m√°s en las predicciones.
5. **M√©todos Embedded (Regularizaci√≥n)**
    
    T√©cnicas como **Lasso (L1)** incorporan la selecci√≥n de *features* directamente en el proceso de entrenamiento del modelo, **penalizando los coeficientes de las variables irrelevantes** hasta reducirlos a cero.
    
    **C√≥mo funciona:**
    
    - **Lasso (L1)**: A√±ade una penalizaci√≥n que fuerza a algunos coeficientes a ser **exactamente cero**, eliminando esas *features* del modelo.
    - **Ridge (L2)**: Reduce el peso de *features* menos importantes, pero **no llega a cero** (solo las "achica").
    
    **Ventajas:**
    
    - **Autom√°tico y eficiente**: Selecciona *features* mientras entrena el modelo.
    - **Robusto contra *overfitting***: Al reducir la complejidad del modelo.
    - **Ideal para modelos lineales** (regresi√≥n lineal, log√≠stica).
    
    **Limitaciones:**
    
    - **Sensible al escalado**: Las *features* deben estar normalizadas (ej: con **`StandardScaler`**).
    - **No captura interacciones complejas** (a menos que se a√±adan manualmente).
    
    **Cu√°ndo usarlo:**
    
    - Cuando tienes **muchas *features*** y sospechas que varias son redundantes o irrelevantes.
    - Para modelos lineales donde la **interpretabilidad** es clave.

---

### üìå Comparaci√≥n de M√©todos

| **M√©todo** | **Ventajas** | **Limitaciones** |
| --- | --- | --- |
| **Varianza** | R√°pido y simple | Solo detecta *features* constantes |
| **Correlaci√≥n** | Elimina redundancia | No considera relaci√≥n con el *target* |
| **SelectKBest** | Basado en estad√≠sticas | Ignora interacciones entre *features* |
| **Importancia (√Årboles)** | Captura relaciones no lineales | Depende del modelo usado |
| **Lasso/Ridge** | Autom√°tico y robusto | Sensible al escalado de *features* |

---

### üöÄ Buenas Pr√°cticas

1. **Empezar con m√©todos simples** (eliminaci√≥n por varianza/correlaci√≥n).
2. **Validar con m√©tricas**: Comparar rendimiento del modelo antes/despu√©s de seleccionar *features*.
3. **Evitar *data leakage*:** Aplicar selecci√≥n solo al conjunto de entrenamiento.

---

## 5. Selecci√≥n y Tuning de Modelo

### üîç Modelos Alternativos

- **Random Forest**:
    - **Ventaja**: Robustez a *outliers* y *overfitting* (promedia m√∫ltiples √°rboles).
    - **Ideal para**: Datos tabulares medianos, *features* mixtas (num√©ricas/categ√≥ricas).
- **XGBoost/LightGBM/CatBoost**:
    
    El¬†**boosting**¬†es una t√©cnica de¬†*machine learning*¬†que combina m√∫ltiples modelos d√©biles (generalmente √°rboles de decisi√≥n simples) en un √∫nico modelo fuerte, entren√°ndolos de forma secuencial. Cada nuevo modelo corrige los errores del anterior, mejorando progresivamente la precisi√≥n. Es especialmente √∫til para problemas de clasificaci√≥n y regresi√≥n, destacando por su alta precisi√≥n y capacidad para manejar datos complejos.
    
    ### **1. XGBoost (eXtreme Gradient Boosting)**
    
    - **Caracter√≠sticas**:
        - Optimizaci√≥n avanzada de¬†*gradient boosting*¬†con regularizaci√≥n (L1/L2).
        - Alto rendimiento y velocidad gracias a paralelizaci√≥n y optimizaci√≥n de hardware.
        - Incluye manejo de valores faltantes y prevenci√≥n de overfitting.
    - **Ventajas**:
        - Muy flexible y eficiente, ideal para competiciones (ej. Kaggle).
        - Soporta m√∫ltiples lenguajes (Python, R, Scala, etc.).
    - **Desventajas**:
        - Mayor consumo de memoria que alternativas m√°s recientes.
    
    **Ejemplo de uso**: Predicci√≥n de riesgo crediticio.
    
    ---
    
    ### **2. LightGBM (Light Gradient Boosting Machine)**
    
    - **Caracter√≠sticas**:
        - Desarrollado por Microsoft, usa¬†*Gradient-Based One-Side Sampling*¬†(GOSS) y¬†*Exclusive Feature Bundling*¬†(EFB) para acelerar el entrenamiento.
        - Basado en √°rboles que crecen *verticalmente*¬†(por hoja), no por niveles.
    - **Ventajas**:
        - Mucho m√°s r√°pido que XGBoost con grandes vol√∫menes de datos.
        - Ideal para datasets con miles de caracter√≠sticas.
    - **Desventajas**:
        - Puede sufrir overfitting en datasets peque√±os.
    
    **Ejemplo de uso**: Clasificaci√≥n de im√°genes m√©dicas.
    
    ---
    
    ### **3. CatBoost (Categorical Boosting)**
    
    - **Caracter√≠sticas**:
        - Dise√±ado para manejar variables categ√≥ricas¬†*sin necesidad de preprocesamiento*¬†(codificaci√≥n autom√°tica).
        - Usa¬†*ordered boosting*¬†para reducir overfitting.
    - **Ventajas**:
        - Excelente con datos categ√≥ricos (ej. variables como "color" o "ciudad").
        - Menor ajuste de hiperpar√°metros vs. XGBoost/LightGBM.
    - **Desventajas**:
        - M√°s lento en entrenamiento que LightGBM.
    
    **Ejemplo de uso**: Sistemas de recomendaci√≥n (ej. productos en e-commerce).
    
    ---
    
- **Redes Neuronales**:
    - **Ventaja**: Captura patrones complejos (ej: im√°genes, texto, series temporales).
    - **Requisito**: Grandes vol√∫menes de datos y poder computacional.

### ‚öôÔ∏è Regularizaci√≥n

**T√©cnicas para evitar *overfitting*:**

- **L1 (Lasso)**: Penaliza coeficientes irrelevantes (los reduce a **0**). ‚Üí Elimina *features*.
- **L2 (Ridge)**: Reduce el peso de *features* sin eliminarlas.
- **Dropout** (redes neuronales): Apaga neuronas aleatoriamente durante el entrenamiento.
- **Early Stopping**: Detiene el entrenamiento si no mejora la m√©trica en validaci√≥n.
- **Pruning** (√°rboles): Elimina ramas poco importantes del √°rbol.

---

### üéõÔ∏è Hyperparameter Tuning

**M√©todos para optimizar par√°metros:**

1. **GridSearchCV**:
    - Prueba **todas las combinaciones** de una grilla predefinida.
    - **Pro**: Encuentra el mejor punto exacto.
    - **Contra**: Costoso computacionalmente.
2. **RandomSearchCV**:
    - Prueba combinaciones **aleatorias** dentro de rangos.
    - **Pro**: M√°s r√°pido y suele encontrar buenos par√°metros.
3. **Optuna/Hyperopt**:
    - Usa **optimizaci√≥n bayesiana** para ajustar par√°metros de forma inteligente.
    - **Pro**: Eficiente con espacios de b√∫squeda grandes.

**Ejemplo de par√°metros a tunear:**

- **Random Forest**: **`n_estimators`**, **`max_depth`**, **`min_samples_split`**.
- **XGBoost**: **`learning_rate`**, **`max_depth`**, **`subsample`**.

---

### ‚ö†Ô∏è Cuidado con Data Leakage

**Errores comunes al tunear:**

- Usar el **mismo dataset** para entrenar y validar (sin separar train/test).
- Aplicar **preprocesamiento** (ej: escalado) antes de la divisi√≥n.
- **Soluci√≥n**: Usar **`Pipeline`** + **`cross_val_score`** para encapsular pasos.

---

### üìå Resumen: Pasos Clave

1. **Seleccionar modelo** seg√∫n tipo de datos y problema.
2. **Aplicar regularizaci√≥n** seg√∫n complejidad del modelo.
3. **Tunear hiperpar√°metros** con m√©todos eficientes (GridSearch, Optuna).
4. **Validar con m√©tricas robustas** y evitar *leakage*.

## 6. Cross Validation

### üìå ¬øPor qu√© usar Cross Validation (CV)?

La validaci√≥n cruzada ayuda a estimar el rendimiento real del modelo sin depender de una sola divisi√≥n train/test, evitando resultados optimistas o pesimistas por azar.

![image.png](image%208.png)

---

### üîç T√©cnicas de CV seg√∫n el Problema

### 1. Stratified K-Fold (Clasificaci√≥n)

- Divide los datos en *K* folds, manteniendo la **misma proporci√≥n de clases** en cada uno.
- **Ideal para**: Datos **desbalanceados** o con clases minoritarias.

**Ejemplo**:

- Si el 20% de los datos son clase "1", cada fold tendr√° ~20% de "1".

**Ventaja**:

- Evita que alg√∫n fold quede sin muestras de una clase.

---

### 2. TimeSeriesSplit (Datos Temporales)

**Qu√© hace**:

- Divide los datos en folds **respetando el orden temporal** (no aleatorio).
- **Ideal para**: Pron√≥sticos (ej: ventas, stock, clima).

**C√≥mo funciona**:

1. Fold 1: Entrenar en [t‚ÇÅ, t‚ÇÇ], validar en t‚ÇÉ.
2. Fold 2: Entrenar en [t‚ÇÅ, t‚ÇÇ, t‚ÇÉ], validar en t‚ÇÑ.
3. Y as√≠ sucesivamente.

**Ventaja**:

- Simula c√≥mo se usar√≠a el modelo en la realidad (prediciendo el futuro).

---

### 3. Group K-Fold (Datos Agrupados)

**Qu√© hace**:

- Asegura que **grupos espec√≠ficos** (ej: mismo paciente, misma tienda) no aparezcan en train y test a la vez.
- **Ideal para**: Evitar *data leakage* en datos correlacionados (ej: m√∫ltiples muestras por paciente).

---

### üìä Evaluaci√≥n de Resultados por Fold

**M√©trica clave**: **Varianza entre folds**.

- **Alta varianza**: El modelo es inestable (depende mucho de la divisi√≥n de datos).
    - **Causas**: Datos insuficientes, *features* poco informativas, *overfitting*.
- **Baja varianza**: Resultados consistentes (modelo robusto).

**Acciones**:

- Si la varianza es alta:
    - Revisar **calidad de los datos** (*outliers*, *features* irrelevantes).
    - Probar **regularizaci√≥n** o modelos m√°s simples.

---

### ‚öôÔ∏è Buenas Pr√°cticas

1. **Elegir el m√©todo de CV seg√∫n la naturaleza de los datos**:
    - Clasificaci√≥n ‚Üí **`StratifiedKFold`**.
    - Series temporales ‚Üí **`TimeSeriesSplit`**.
2. **Usar suficientes folds** (t√≠picamente *K=5 o K=10*).
    - M√°s folds = Mayor costo computacional, pero menor varianza en la estimaci√≥n.
3. **Comparar m√©tricas entre folds**:
    - Si un fold tiene un rendimiento muy bajo, investigar qu√© lo causa.

---

## 7. Interpretaci√≥n del Modelo

### üìå ¬øPor qu√© es importante?

Entender **c√≥mo** el modelo toma decisiones ayuda a:

- Validar que sus predicciones sean l√≥gicas (no basadas en artefactos o sesgos).
- Comunicar resultados a *stakeholders* no t√©cnicos.
- Cumplir regulaciones (ej: RGPD, "derecho a explicaci√≥n").

![image.png](image%209.png)

---

### üîç M√©todos Comunes

### 1. Feature Importance (Importancia de Variables)

**Qu√© muestra**:

- El **impacto relativo** de cada *feature* en las predicciones.
- **Modelos compatibles**: √Årboles (Random Forest, XGBoost), modelos lineales.

**Limitaciones**:

- No explica **c√≥mo** afecta cada variable (solo "cu√°nto").
- Puede ser enga√±osa si hay *features* correlacionadas.

---

### 2. SHAP (SHapley Additive exPlanations)

**Qu√© hace**:

- Explica cada predicci√≥n individual como una **suma de contribuciones** de cada *feature*.
- **Ventajas**:
    - Funciona para **cualquier modelo** (incluso redes neuronales).
    - Captura interacciones entre variables.

**Visualizaciones √∫tiles**:

- **Summary plot**: Muestra el impacto global de cada *feature*.
- **Force plot**: Explica una predicci√≥n espec√≠fica.

![image.png](image%2010.png)

---

### 3. LIME (Local Interpretable Model-agnostic Explanations)

**Qu√© hace**:

- Crea una **aproximaci√≥n local** (modelo lineal simple) alrededor de una predicci√≥n.
- **Ventaja**: F√°cil de interpretar para casos concretos.
- **Desventaja**: Solo v√°lido para peque√±as regiones del espacio de *features*.

---

### üìä Interpretaci√≥n seg√∫n Tipo de Modelo

| **Modelo** | **Mejor M√©todo** | **Ejemplo de Uso** |
| --- | --- | --- |
| **√Årboles** | Feature Importance + SHAP | "La edad es 3x m√°s importante que el ingreso" |
| **Modelos Lineales** | Coeficientes | "Aumentar X en 1 unidad sube Y en 0.5" |
| **Redes Neuronales** | SHAP/LIME | "El p√≠xel X activ√≥ esta neurona" |

---

### ‚ö†Ô∏è Errores Comunes

1. **Ignorar el contexto del negocio**: Una *feature* puede ser importante matem√°ticamente pero no tener sentido real.
2. **Sobresimplificar**: Asumir que "importancia" = "causalidad" (¬°correlaci√≥n ‚â† causalidad!).

---

## 8. Validaci√≥n final y stress testing

### üîç Objetivo

Garantizar que el modelo funciona **en condiciones reales** y es robusto frente a datos inesperados o adversos.

---

### üìå Evaluaci√≥n con Holdout no Visto

**Qu√© es**:

- Probar el modelo en un **dataset separado desde el inicio** (nunca usado en entrenamiento, validaci√≥n o *tuning*).
- **Recomendaciones**:
    - El *holdout* debe reflejar la distribuci√≥n real de los datos (ej: misma proporci√≥n de clases).
    - Usar m√©tricas **consistentes** con las del desarrollo (ej: si optimizaste F1, reporta F1 aqu√≠).

**Acciones clave**:

- Comparar m√©tricas vs. los resultados de validaci√≥n cruzada.
    - Si hay **discrepancia grande**: Posible *data leakage* o *overfitting*.

---

### üß™ Stress Testing

Pruebas para evaluar la robustez del modelo en escenarios adversos o l√≠mite:

### 1. Datos con Ruido o Errores

**Qu√© hacer**:

- Inyectar ruido aleatorio (ej: modificar un 10% de los valores) o *outliers* artificiales.
- **M√©trica a observar**: Cambio en el rendimiento (ej: ¬øel accuracy cae dr√°sticamente?).

**Ejemplo**:

- Modelo de fraudes: ¬øSigue detectando fraudes si los montos tienen errores de tipeo?

### 2. Datos Extremos (Edge Cases)

**Qu√© hacer**:

- Probar con valores en los l√≠mites del rango conocido (ej: edad = 0 o 120 a√±os).
- **Pregunta clave**: ¬øLas predicciones son l√≥gicas o el modelo "alucina"?

**Ejemplo**:

- Modelo de pr√©stamos: ¬øQu√© pasa si un cliente tiene ingresos 10x mayores que el m√°ximo visto en entrenamiento?

### 3. Segmentaci√≥n por Subgrupos

**Qu√© hacer**:

- Evaluar el modelo en **subconjuntos espec√≠ficos** (ej: por regi√≥n, g√©nero, rango etario).
- **Objetivo**: Detectar sesgos o bajo rendimiento en grupos minoritarios.

**Ejemplo**:

- Si el modelo para aprobar cr√©ditos tiene 90% accuracy global, pero 60% en mujeres j√≥venes, hay un problema.

---

### üìä M√©tricas Clave para Stress Testing

| **Prueba** | **Qu√© Medir** | **Alerta** |
| --- | --- | --- |
| **Holdout no visto** | Accuracy/Precision/Recall vs. validaci√≥n | Diferencias > 5-10% |
| **Ruido** | Degradaci√≥n de m√©tricas | Ca√≠da abrupta (>15-20%) |
| **Segmentos** | Rendimiento por grupo | Diferencias injustificadas entre grupos |

---

### üö® Errores Comunes

- **No probar con datos realistas**: Ej: usar *holdout* con distribuci√≥n diferente a la producci√≥n.
- **Ignorar falsos positivos/negativos costosos**: Ej: en medicina, un falso negativo puede ser grave.

---

### ‚úÖ Checklist Post-Validaci√≥n

1. **Holdout**: ¬øLas m√©tricas son similares a las de entrenamiento?
2. **Ruido**: ¬øEl modelo es estable ante perturbaciones?
3. **Sesgos**: ¬øHay grupos con peor rendimiento?
4. **Business Logic**: ¬øLas predicciones extremas tienen sentido?

---

## 9. Preparaci√≥n para producci√≥n

### üì¶ Exportar el Modelo

**Formatos recomendados**:

- **`joblib`/`pickle`** (Python):
    - Ideal para modelos de **`sklearn`** y datos peque√±os.
- **`ONNX`** (Interoperabilidad):
    - √ötil para desplegar en entornos no-Python (ej: C++, m√≥viles).
    - Requiere conversi√≥n previa (ej: **`sklearn-onnx`**).

**Advertencias**:

- Verificar compatibilidad de versiones (¬°el entorno de producci√≥n debe usar las mismas librer√≠as!).
- Incluir *metadata* (ej: versi√≥n del modelo, fecha de entrenamiento).
    
    ---
    
    ### üîß Pipeline Reproducible
    
    **Objetivo**: Encapsular **todas las transformaciones** (preprocesamiento + modelo) en un solo objeto.
    
    **Ventajas**:
    
    - Evita *data leakage* (transformaciones se aplican correctamente en producci√≥n).
    - F√°cil de versionar y compartir.
    
    **Recomendaci√≥n**:
    
    - Incluir incluso pasos "simples" (ej: **`OneHotEncoder`**, imputaci√≥n de nulos).
    
    ---
    
    ### üìú Scoring Script (API B√°sica)
    
    **Estructura t√≠pica**:
    
    1. **Cargar** el modelo/pipeline.
    2. **Validar** el *input* (ej: columnas, tipos de datos).
    3. **Preprocesar** (usando el pipeline).
    4. **Predecir** y devolver el *output*.
    
    **Requisitos clave**:
    
    - **Logging**: Registrar *inputs*, errores y predicciones.
    - **Performance**: Optimizar para baja latencia (ej: evitar c√°lculos pesados en caliente).
    
    ---
    
    ### üîç Validaci√≥n Input/Output
    
    **Qu√© chequear**:
    
    1. **Input**:
        - Columnas requeridas y tipos correctos (ej: **`int`** vs **`float`**).
        - Rango de valores (ej: edad no negativa).
    2. **Output**:
        - Formato consistente (ej: diccionario JSON con claves fijas).
        - Valores posibles (ej: clases **`[0, 1]`** en clasificaci√≥n binaria).
    
    **Herramientas**:
    
    - **`pydantic`**: Para validar esquemas de datos en Python.
    - **Tests unitarios**: Verificar *edge cases* (ej: *inputs* vac√≠os).
    
    ---
    
    ### üìå Checklist para Producci√≥n
    
    | **Paso** | **Acci√≥n** |
    | --- | --- |
    | **Exportar modelo** | Guardar en formato est√°ndar (joblib/ONNX). |
    | **Pipeline** | Incluir todo el preprocesamiento. |
    | **Scoring script** | Validar input, logging, output claro. |
    | **Validaci√≥n** | Probar con datos similares a producci√≥n. |
    
    ---
    
    ---
    

## 10. Puesta en producci√≥n

Una vez que tienes tu modelo entrenado, validado y empaquetado en un¬†**pipeline reproducible**, el siguiente paso es desplegarlo en un entorno real donde pueda recibir datos y devolver predicciones. Aqu√≠ te explico c√≥mo hacerlo de forma eficiente.

![image.png](image%2011.png)

---

### **1. Elegir el Tipo de Despliegue**

Dependiendo de tus necesidades, puedes optar por:

- **API REST**¬†(ej: con FastAPI o Flask):
    - Ideal para integraci√≥n con aplicaciones web/m√≥viles.
    - Escalable con servicios como AWS Lambda o Google Cloud Run.
- **Batch (Procesamiento por Lotes)**:
    - √ötil cuando las predicciones no son en tiempo real (ej: procesar datos cada noche).
    - Se usa con herramientas como¬†**Airflow**¬†o scripts programados (Cron).
- **Embedded (Empotrado en Dispositivos)**:
    - Si el modelo debe correr localmente (ej: apps m√≥viles, IoT).
    - Formatos como¬†**ONNX**¬†o¬†**TensorFlow Lite**¬†optimizan el rendimiento.

---

### **2. Entorno de Ejecuci√≥n**

- **Contenedores (Docker)**:
    - Empaqueta el modelo, las dependencias y el script en un entorno aislado.
    - F√°cil de desplegar en Kubernetes o servicios en la nube.
- **Serverless (AWS Lambda, Google Cloud Functions)**:
    - Ideal para cargas variables (escala autom√°ticamente).
    - Bajo costo si el tr√°fico es irregular.
- **Servidores Dedicados (EC2, VM)**:
    - Para modelos pesados que necesitan GPU o alta memoria.

---

### **3. Monitoreo y Mantenimiento**

Una vez en producci√≥n, es crucial:

‚úÖ¬†**Seguimiento del Rendimiento**:

- ¬øLas predicciones siguen siendo precisas? (Comparar con datos reales).
- Alertas si la tasa de error aumenta (**Data Drift**).

‚úÖ¬†**Escalabilidad**:

- ¬øEl sistema aguanta picos de tr√°fico? (Usar balanceadores de carga).

‚úÖ¬†**Registro de Errores (Logging)**:

- Guardar inputs fallidos para an√°lisis posterior.

‚úÖ¬†**Actualizaciones**:

- Reentrenar el modelo peri√≥dicamente con nuevos datos.

---

### **4. Seguridad y Robustez**

üîí¬†**Protecci√≥n de la API**:

- Usar autenticaci√≥n (API Keys, JWT).
- Limitar peticiones por segundo (Rate Limiting).

üõ°Ô∏è¬†**Validaci√≥n de Inputs**:

- Rechazar datos malformados o sospechosos (ej: inyecci√≥n SQL).

üíæ¬†**Backups y Recovery**:

- Tener copias del modelo y datos por si falla el servidor.

---

### **üìã Checklist Final para Producci√≥n**

| **Aspecto** | **Acci√≥n** |
| --- | --- |
| **Tipo de Despliegue** | Elegir entre API, Batch o Embedded seg√∫n necesidades. |
| **Entorno** | Docker, Serverless o Servidor dedicado. |
| **Monitoreo** | Implementar logs, alertas y m√©tricas de rendimiento. |
| **Seguridad** | Autenticaci√≥n, rate limiting y validaci√≥n de inputs. |
| **Mantenimiento** | Planificar retraining y actualizaciones. |

---

### **üîó Ejemplo de Flujo en Producci√≥n**

1. **Usuario**¬†env√≠a datos ‚Üí¬†**API REST**¬†(FastAPI).
2. **API**¬†valida el input ‚Üí pasa al¬†**Pipeline**¬†guardado.
3. **Modelo**¬†predice ‚Üí devuelve resultado en JSON.
4. **Sistema**¬†registra la petici√≥n y monitorea el rendimiento.

## 11. Monitoreo en producci√≥n

![image.png](image%2012.png)

### üìä M√©tricas Clave a Monitorear

1. **Performance del Modelo**:
    - **Accuracy/Precision/Recall**: ¬øSe mantienen estables?
    - **Comparar con l√≠nea base** (ej: rendimiento en validaci√≥n inicial).
    - **Alertas**: Ca√≠das s√∫bitas (>10-15%).
2. **Data Drift**:
    - Cambios en la **distribuci√≥n de los datos de entrada** (*features*).
    - M√©tricas:
        - **KS-test** (Kolmog√≥rov-Smirnov) para variables num√©ricas.
        - **Chi-cuadrado** para categ√≥ricas.
3. **Concept Drift**:
    - Cambios en la **relaci√≥n entre *features* y *target***.
    - Ejemplo: Un modelo de spam entrenado en 2020 puede fallar en 2024.
    - Detecci√≥n:
        - Monitorear **ratio de predicciones positivas/negativas** en el tiempo.
        - Comparar **acuerdo entre predicciones y resultados reales** (si hay feedback).

### üìå T√©cnicas de Monitoreo

### 1. Guardar Predicciones y Ground Truth

- **Base de datos de logs**: Almacenar *inputs*, predicciones y resultados reales (si est√°n disponibles).
- **Ejemplo**:
    
    ```python
    log_entry = {
        'timestamp': '2024-05-20 12:00:00',
        'input_data': {'age': 35, 'income': 50000},
        'prediction': 1,
        'actual': None  # Actualizar cuando se sepa
    }
    
    ```
    
- **Uso**:
    - Calcular m√©tricas retrasadas (ej: precision real vs. estimada).
    - Detectar *concept drift*.

### 2. Herramientas Automatizadas

- **Librer√≠as**:
    - **Evidently**: Reportes de *drift* y calidad de datos.
    - **Alibi Detect**: Detecci√≥n de *outliers* y *drift*.
- **Dashboards**: Grafana, MLflow, o soluciones *custom*.

### 3. Checks en Tiempo Real

- **Validar *inputs***:
    - ¬øLas *features* tienen valores dentro de los rangos esperados?
    - ¬øHay nuevas categor√≠as no vistas en entrenamiento?

---

### üö® Escenarios Cr√≠ticos

| **Problema** | **Se√±ales** | **Acciones** |
| --- | --- | --- |
| **Data Drift** | Distribuci√≥n de *features* cambia | Reentrenar o ajustar preprocesamiento |
| **Concept Drift** | M√©tricas caen pero datos son similares | Recolectar nuevos datos y reentrenar |
| **Performance Decay** | Accuracy baja gradualmente | Investigar causas (ej: cambios en el negocio) |

---

### ‚úÖ Checklist de Monitoreo

1. **Definir m√©tricas clave** y umbrales de alerta.
2. **Automatizar** reportes diarios/semanales.
3. **Plan de acci√≥n** para *drift* (ej: reentrenamiento cada 3 meses).
